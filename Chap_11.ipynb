{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing and Exploding Gradient\n",
    "\n",
    "Gradients often get smaller and smaller as the algorithm progresses down to the lower layers.\n",
    "\n",
    "Looking at the logistic activation function, you can see that when\n",
    "inputs become large (negative or positive), the function saturates at 0 or 1, with a\n",
    "derivative extremely close to 0. \n",
    "\n",
    "![title](../Images/Hands_on_ml/sigmoid.png)\n",
    "\n",
    "### Xavier and He initialization\n",
    "\n",
    "In their paper, Glorot and Bengio propose a way to significantly alleviate this problem\n",
    "\n",
    "The authors argue that we need the variance of the outputs of each layer to be equal to the variance of its inputs\n",
    "\n",
    "![title](../Images/Hands_on_ml/11_func_table.png)\n",
    "\n",
    "### Nonsaturating Activation Functions\n",
    "\n",
    "One of the insights in the 2010 paper by Glorot and Bengio was that the vanishing/exploding \n",
    "gradients problems were in part due to a poor choice of activation function\n",
    "\n",
    "Relu suffers from a problem known as the dying ReLUs: during training, some neurons effectively die, meaning they stop outputting anything other than 0. \n",
    "\n",
    "Especially if you used a large learning rate\n",
    "\n",
    "To solve this problem, you may want to use a variant of the ReLU function, such as the leaky ReLU. \n",
    "\n",
    "This function is defined as LeakyReLU α(z) = max(αz, z)\n",
    "\n",
    "![title](../Images/Hands_on_ml/11_func_leakyrelu.png)\n",
    "\n",
    "Setting α = 0.2 (huge leak) seemed to result in better performance than α = 0.01 (small leak).\n",
    "\n",
    "The parametric leaky ReLU (PReLU), where α is authorized to be learned during training (instead of being a hyperparameter, it becomes a parameter that can be modified by backpropagation like any other parameter).\n",
    "\n",
    "This was reported to strongly outperform ReLU on large image datasets, but on smaller\n",
    "datasets it runs the risk of overfitting the training set.\n",
    "\n",
    "![title](../Images/Hands_on_ml/11_func_ELU.png)\n",
    "\n",
    "Exponential linear unit (ELU) that outperformed all the ReLU variants in their experiments: training time was reduced and the neural network performed better on the test set. \n",
    "\n",
    "Although your mileage will vary, in general ELU > leaky ReLU (and its variants) > ReLU > tanh > logis‐\n",
    "tic. If you care a lot about runtime performance, then you may prefer leaky ReLUs over ELUs. \n",
    "\n",
    "If you don’t want to tweak yet another hyperparameter, you may just use the default α values suggested\n",
    "earlier (0.01 for the leaky ReLU, and 1 for ELU)\n",
    "\n",
    "### Batch Normalization\n",
    "\n",
    "Batch Normalization is a tool that address the vanishing/exploding gradients problems, and more generally the problem that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change .\n",
    "\n",
    "In other words, this operation lets the model learn the optimal scale and mean of the inputs for each layer.\n",
    "\n",
    "![title](../Images/Hands_on_ml/11_Batch_normalize.png)\n",
    "\n",
    "Very good for estimation but, training time and execution time goes worse\n",
    "\n",
    "You can use Batch Normalization tf.layer.batch.normalization()\n",
    "\n",
    "You can use functool's partial module for shrinking repetition\n",
    "\n",
    "Whenever you run an operation that depends on the batch_norm layer, you need to set the is_train\n",
    "ing placeholder to True or False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
